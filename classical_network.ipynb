{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Objective function from Information Theory perspective*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data loading from MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision\n",
    "\n",
    "train_dataset = dataset.MNIST(root = './data', train = True, transform = transforms.ToTensor(), download = True )\n",
    "test_dataset = dataset.MNIST(root = './data', train = False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Network model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as fun\n",
    "import torch.nn as nn\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_dim, nh1, nh2, nh3, nh4, nh5, output_dim):\n",
    "        super(DeepNN, self).__init__()\n",
    "        #hyperparameter setting \n",
    "        self.input_dim = input_dim\n",
    "        self.nh1, self.nh2, self.nh3, self.nh4, self.nh5, = nh1, nh2, nh3, nh4, nh5\n",
    "        self.output_dim = output_dim\n",
    "        #layer definition \n",
    "        self.input_layer = nn.Linear(self.input_dim, self.nh1)\n",
    "        self.hlayer1 = nn.Linear(self.nh1, self.nh2)\n",
    "        self.hlayer2 = nn.Linear(self.nh2, self.nh3)\n",
    "        self.hlayer3 = nn.Linear(self.nh3, self.nh4)\n",
    "        self.hlayer4 = nn.Linear(self.nh4, self.nh5)\n",
    "        self.output_layer = nn.Linear(self.nh5, self.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #propogation of each layer\n",
    "        self.out1 = fun.relu(self.input_layer(x))\n",
    "        self.out2 = fun.relu(self.hlayer1(self.out1))\n",
    "        self.out3 = fun.relu(self.hlayer2(self.out2))\n",
    "        self.out4 = fun.relu(self.hlayer3(self.out3))\n",
    "        self.out5 = fun.relu(self.hlayer4(self.out4))\n",
    "        self.out6 = fun.relu(self.output_layer(self.out5))\n",
    "        return fun.log_softmax(self.out6, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = DeepNN(784, 1000, 1200, 1100, 1000, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obj.cuda()          #if GPU available enable CUDA extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define Loss criterion and optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "criterion = nn.CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(obj.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [100/600], Loss: 1.0506\n",
      "Epoch [1/30], Step [200/600], Loss: 0.8101\n",
      "Epoch [1/30], Step [300/600], Loss: 0.8304\n",
      "Epoch [1/30], Step [400/600], Loss: 0.6280\n",
      "Epoch [1/30], Step [500/600], Loss: 0.6061\n",
      "Epoch [1/30], Step [600/600], Loss: 0.5799\n",
      "Epoch 1/30 Loss: 0.75086\n",
      " --------------------- \n",
      "Epoch [2/30], Step [100/600], Loss: 0.5464\n",
      "Epoch [2/30], Step [200/600], Loss: 0.5816\n",
      "Epoch [2/30], Step [300/600], Loss: 0.6981\n",
      "Epoch [2/30], Step [400/600], Loss: 0.6173\n",
      "Epoch [2/30], Step [500/600], Loss: 0.6299\n",
      "Epoch [2/30], Step [600/600], Loss: 0.6375\n",
      "Epoch 2/30 Loss: 0.61846\n",
      " --------------------- \n",
      "Epoch [3/30], Step [100/600], Loss: 0.3992\n",
      "Epoch [3/30], Step [200/600], Loss: 0.6102\n",
      "Epoch [3/30], Step [300/600], Loss: 0.5729\n",
      "Epoch [3/30], Step [400/600], Loss: 0.4070\n",
      "Epoch [3/30], Step [500/600], Loss: 0.4510\n",
      "Epoch [3/30], Step [600/600], Loss: 0.4453\n",
      "Epoch 3/30 Loss: 0.48094\n",
      " --------------------- \n",
      "Epoch [4/30], Step [100/600], Loss: 0.4756\n",
      "Epoch [4/30], Step [200/600], Loss: 0.5088\n",
      "Epoch [4/30], Step [300/600], Loss: 0.6078\n",
      "Epoch [4/30], Step [400/600], Loss: 0.3986\n",
      "Epoch [4/30], Step [500/600], Loss: 0.4251\n",
      "Epoch [4/30], Step [600/600], Loss: 0.3716\n",
      "Epoch 4/30 Loss: 0.46459\n",
      " --------------------- \n",
      "Epoch [5/30], Step [100/600], Loss: 0.4472\n",
      "Epoch [5/30], Step [200/600], Loss: 0.5415\n",
      "Epoch [5/30], Step [300/600], Loss: 0.4445\n",
      "Epoch [5/30], Step [400/600], Loss: 0.4897\n",
      "Epoch [5/30], Step [500/600], Loss: 0.3926\n",
      "Epoch [5/30], Step [600/600], Loss: 0.5476\n",
      "Epoch 5/30 Loss: 0.47719\n",
      " --------------------- \n",
      "Epoch [6/30], Step [100/600], Loss: 0.5298\n",
      "Epoch [6/30], Step [200/600], Loss: 0.5885\n",
      "Epoch [6/30], Step [300/600], Loss: 0.4738\n",
      "Epoch [6/30], Step [400/600], Loss: 0.4789\n",
      "Epoch [6/30], Step [500/600], Loss: 0.7085\n",
      "Epoch [6/30], Step [600/600], Loss: 0.5185\n",
      "Epoch 6/30 Loss: 0.54967\n",
      " --------------------- \n",
      "Epoch [7/30], Step [100/600], Loss: 0.4838\n",
      "Epoch [7/30], Step [200/600], Loss: 0.5554\n",
      "Epoch [7/30], Step [300/600], Loss: 0.5817\n",
      "Epoch [7/30], Step [400/600], Loss: 0.6627\n",
      "Epoch [7/30], Step [500/600], Loss: 0.5167\n",
      "Epoch [7/30], Step [600/600], Loss: 0.5487\n",
      "Epoch 7/30 Loss: 0.55816\n",
      " --------------------- \n",
      "Epoch [8/30], Step [100/600], Loss: 0.3352\n",
      "Epoch [8/30], Step [200/600], Loss: 0.4195\n",
      "Epoch [8/30], Step [300/600], Loss: 0.5738\n",
      "Epoch [8/30], Step [400/600], Loss: 0.4897\n",
      "Epoch [8/30], Step [500/600], Loss: 0.3305\n",
      "Epoch [8/30], Step [600/600], Loss: 0.3577\n",
      "Epoch 8/30 Loss: 0.41771\n",
      " --------------------- \n",
      "Epoch [9/30], Step [100/600], Loss: 0.6823\n",
      "Epoch [9/30], Step [200/600], Loss: 0.4609\n",
      "Epoch [9/30], Step [300/600], Loss: 0.4738\n",
      "Epoch [9/30], Step [400/600], Loss: 0.4015\n",
      "Epoch [9/30], Step [500/600], Loss: 0.6806\n",
      "Epoch [9/30], Step [600/600], Loss: 0.4442\n",
      "Epoch 9/30 Loss: 0.52388\n",
      " --------------------- \n",
      "Epoch [10/30], Step [100/600], Loss: 0.5297\n",
      "Epoch [10/30], Step [200/600], Loss: 0.3364\n",
      "Epoch [10/30], Step [300/600], Loss: 0.4419\n",
      "Epoch [10/30], Step [400/600], Loss: 0.3972\n",
      "Epoch [10/30], Step [500/600], Loss: 0.2568\n",
      "Epoch [10/30], Step [600/600], Loss: 0.4606\n",
      "Epoch 10/30 Loss: 0.40377\n",
      " --------------------- \n",
      "Epoch [11/30], Step [100/600], Loss: 0.5236\n",
      "Epoch [11/30], Step [200/600], Loss: 0.4230\n",
      "Epoch [11/30], Step [300/600], Loss: 0.4468\n",
      "Epoch [11/30], Step [400/600], Loss: 0.4614\n",
      "Epoch [11/30], Step [500/600], Loss: 0.3358\n",
      "Epoch [11/30], Step [600/600], Loss: 0.4468\n",
      "Epoch 11/30 Loss: 0.43959\n",
      " --------------------- \n",
      "Epoch [12/30], Step [100/600], Loss: 0.4405\n",
      "Epoch [12/30], Step [200/600], Loss: 0.4839\n",
      "Epoch [12/30], Step [300/600], Loss: 0.5250\n",
      "Epoch [12/30], Step [400/600], Loss: 0.2837\n",
      "Epoch [12/30], Step [500/600], Loss: 0.5942\n",
      "Epoch [12/30], Step [600/600], Loss: 0.5616\n",
      "Epoch 12/30 Loss: 0.48148\n",
      " --------------------- \n",
      "Epoch [13/30], Step [100/600], Loss: 0.4380\n",
      "Epoch [13/30], Step [200/600], Loss: 0.4624\n",
      "Epoch [13/30], Step [300/600], Loss: 0.6462\n",
      "Epoch [13/30], Step [400/600], Loss: 0.4606\n",
      "Epoch [13/30], Step [500/600], Loss: 0.3374\n",
      "Epoch [13/30], Step [600/600], Loss: 0.5780\n",
      "Epoch 13/30 Loss: 0.48710\n",
      " --------------------- \n",
      "Epoch [14/30], Step [100/600], Loss: 0.5410\n",
      "Epoch [14/30], Step [200/600], Loss: 0.5109\n",
      "Epoch [14/30], Step [300/600], Loss: 0.4643\n",
      "Epoch [14/30], Step [400/600], Loss: 0.2391\n",
      "Epoch [14/30], Step [500/600], Loss: 0.3004\n",
      "Epoch [14/30], Step [600/600], Loss: 0.3842\n",
      "Epoch 14/30 Loss: 0.40666\n",
      " --------------------- \n",
      "Epoch [15/30], Step [100/600], Loss: 0.1924\n",
      "Epoch [15/30], Step [200/600], Loss: 0.1612\n",
      "Epoch [15/30], Step [300/600], Loss: 0.3722\n",
      "Epoch [15/30], Step [400/600], Loss: 0.2324\n",
      "Epoch [15/30], Step [500/600], Loss: 0.1833\n",
      "Epoch [15/30], Step [600/600], Loss: 0.2250\n",
      "Epoch 15/30 Loss: 0.22777\n",
      " --------------------- \n",
      "Epoch [16/30], Step [100/600], Loss: 0.2073\n",
      "Epoch [16/30], Step [200/600], Loss: 0.2130\n",
      "Epoch [16/30], Step [300/600], Loss: 0.1639\n",
      "Epoch [16/30], Step [400/600], Loss: 0.3036\n",
      "Epoch [16/30], Step [500/600], Loss: 0.2108\n",
      "Epoch [16/30], Step [600/600], Loss: 0.2485\n",
      "Epoch 16/30 Loss: 0.22452\n",
      " --------------------- \n",
      "Epoch [17/30], Step [100/600], Loss: 0.3644\n",
      "Epoch [17/30], Step [200/600], Loss: 0.2706\n",
      "Epoch [17/30], Step [300/600], Loss: 0.3164\n",
      "Epoch [17/30], Step [400/600], Loss: 0.1403\n",
      "Epoch [17/30], Step [500/600], Loss: 0.2006\n",
      "Epoch [17/30], Step [600/600], Loss: 0.2141\n",
      "Epoch 17/30 Loss: 0.25107\n",
      " --------------------- \n",
      "Epoch [18/30], Step [100/600], Loss: 0.2197\n",
      "Epoch [18/30], Step [200/600], Loss: 0.2383\n",
      "Epoch [18/30], Step [300/600], Loss: 0.2114\n",
      "Epoch [18/30], Step [400/600], Loss: 0.4878\n",
      "Epoch [18/30], Step [500/600], Loss: 0.1151\n",
      "Epoch [18/30], Step [600/600], Loss: 0.2082\n",
      "Epoch 18/30 Loss: 0.24676\n",
      " --------------------- \n",
      "Epoch [19/30], Step [100/600], Loss: 0.1392\n",
      "Epoch [19/30], Step [200/600], Loss: 0.2995\n",
      "Epoch [19/30], Step [300/600], Loss: 0.3054\n",
      "Epoch [19/30], Step [400/600], Loss: 0.2097\n",
      "Epoch [19/30], Step [500/600], Loss: 0.2346\n",
      "Epoch [19/30], Step [600/600], Loss: 0.2469\n",
      "Epoch 19/30 Loss: 0.23921\n",
      " --------------------- \n",
      "Epoch [20/30], Step [100/600], Loss: 0.1619\n",
      "Epoch [20/30], Step [200/600], Loss: 0.2442\n",
      "Epoch [20/30], Step [300/600], Loss: 0.1182\n",
      "Epoch [20/30], Step [400/600], Loss: 0.2342\n",
      "Epoch [20/30], Step [500/600], Loss: 0.2561\n",
      "Epoch [20/30], Step [600/600], Loss: 0.1445\n",
      "Epoch 20/30 Loss: 0.19316\n",
      " --------------------- \n",
      "Epoch [21/30], Step [100/600], Loss: 0.2663\n",
      "Epoch [21/30], Step [200/600], Loss: 0.2534\n",
      "Epoch [21/30], Step [300/600], Loss: 0.2202\n",
      "Epoch [21/30], Step [400/600], Loss: 0.2406\n",
      "Epoch [21/30], Step [500/600], Loss: 0.1767\n",
      "Epoch [21/30], Step [600/600], Loss: 0.2167\n",
      "Epoch 21/30 Loss: 0.22897\n",
      " --------------------- \n",
      "Epoch [22/30], Step [100/600], Loss: 0.2329\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "epochs = 30\n",
    "train_loss = []\n",
    "for epoch in range(epochs):\n",
    "    loss_monitor = []\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        pred = obj(images)\n",
    "        loss = fun.nll_loss(pred, labels)\n",
    "        #loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:                              \n",
    "            loss_monitor.append(loss.item())\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'%(epoch+1, epochs, i+1, len(train_dataset)//batch_size, loss.item()))\n",
    "    print('Epoch %d/%d Loss: %.5f'%(epoch+1,epochs,sum(loss_monitor)/len(loss_monitor)))\n",
    "    train_loss.append(sum(loss_monitor)/len(loss_monitor))\n",
    "    print(\" --------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "figure = plt.figure()\n",
    "plt.plot(range(epochs), train_loss, color = 'blue')\n",
    "plt.legend([train_loss], loc = 'upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "figure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
