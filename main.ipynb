{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Objective function from Information Theory perspective*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data loading from MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dataset\n",
    "import torchvision\n",
    "\n",
    "train_dataset = dataset.MNIST(root = './data', train = True, transform = transforms.ToTensor(), download = True )\n",
    "test_dataset = dataset.MNIST(root = './data', train = False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### define dataloaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Network model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as fun\n",
    "import torch.nn as nn\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_dim, nh1, nh2, nh3, nh4, nh5, output_dim):\n",
    "        super(DeepNN, self).__init__()\n",
    "        #hyperparameter setting \n",
    "        self.input_dim = input_dim\n",
    "        self.nh1, self.nh2, self.nh3, self.nh4, self.nh5, = nh1, nh2, nh3, nh4, nh5\n",
    "        self.output_dim = output_dim\n",
    "        #layer definition \n",
    "        self.input_layer = nn.Linear(self.input_dim, self.nh1)\n",
    "        self.hlayer1 = nn.Linear(self.nh1, self.nh2)\n",
    "        self.hlayer2 = nn.Linear(self.nh2, self.nh3)\n",
    "        self.hlayer3 = nn.Linear(self.nh3, self.nh4)\n",
    "        self.hlayer4 = nn.Linear(self.nh4, self.nh5)\n",
    "        self.output_layer = nn.Linear(self.nh5, self.output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #propogation of each layer\n",
    "        self.out1 = fun.relu(self.input_layer(x))\n",
    "        self.out2 = fun.relu(self.hlayer1(self.out1))\n",
    "        self.out3 = fun.relu(self.hlayer2(self.out2))\n",
    "        self.out4 = fun.relu(self.hlayer3(self.out3))\n",
    "        self.out5 = fun.relu(self.hlayer4(self.out4))\n",
    "        self.out6 = fun.relu(self.output_layer(self.out5))\n",
    "        print(fun.log_softmax(self.out6, dim = 1))\n",
    "        return fun.log_softmax(self.out6, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = DeepNN(784, 1000, 1200, 1100, 1000, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obj.cuda()          #if GPU available enable CUDA extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define Loss criterion and optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "criterion = nn.CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(obj.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3306, -2.3306, -2.3306, -2.3306, -2.2740, -2.3306, -2.3306, -2.2516,\n",
      "         -2.2610, -2.2615],\n",
      "        [-2.3309, -2.3309, -2.3309, -2.3309, -2.2753, -2.3309, -2.3309, -2.2477,\n",
      "         -2.2610, -2.2625],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2732, -2.3306, -2.3306, -2.2493,\n",
      "         -2.2620, -2.2636],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2749, -2.3306, -2.3306, -2.2494,\n",
      "         -2.2616, -2.2623],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2761, -2.3303, -2.3303, -2.2497,\n",
      "         -2.2613, -2.2630],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2741, -2.3304, -2.3304, -2.2497,\n",
      "         -2.2630, -2.2627],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2770, -2.3304, -2.3304, -2.2459,\n",
      "         -2.2615, -2.2648],\n",
      "        [-2.3299, -2.3299, -2.3299, -2.3299, -2.2764, -2.3299, -2.3299, -2.2498,\n",
      "         -2.2611, -2.2645],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2749, -2.3305, -2.3305, -2.2485,\n",
      "         -2.2632, -2.2621],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2741, -2.3308, -2.3308, -2.2502,\n",
      "         -2.2614, -2.2614],\n",
      "        [-2.3311, -2.3311, -2.3311, -2.3311, -2.2742, -2.3311, -2.3311, -2.2474,\n",
      "         -2.2611, -2.2624],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2752, -2.3303, -2.3303, -2.2491,\n",
      "         -2.2616, -2.2637],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2752, -2.3304, -2.3304, -2.2504,\n",
      "         -2.2608, -2.2628],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2746, -2.3305, -2.3305, -2.2492,\n",
      "         -2.2627, -2.2621],\n",
      "        [-2.3309, -2.3309, -2.3309, -2.3309, -2.2750, -2.3309, -2.3309, -2.2483,\n",
      "         -2.2607, -2.2627],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2770, -2.3303, -2.3303, -2.2484,\n",
      "         -2.2611, -2.2632],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2739, -2.3308, -2.3308, -2.2491,\n",
      "         -2.2617, -2.2623],\n",
      "        [-2.3309, -2.3309, -2.3309, -2.3309, -2.2746, -2.3309, -2.3309, -2.2492,\n",
      "         -2.2618, -2.2611],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2759, -2.3306, -2.3306, -2.2487,\n",
      "         -2.2603, -2.2630],\n",
      "        [-2.3301, -2.3301, -2.3301, -2.3301, -2.2756, -2.3301, -2.3301, -2.2492,\n",
      "         -2.2626, -2.2633],\n",
      "        [-2.3311, -2.3311, -2.3311, -2.3311, -2.2754, -2.3311, -2.3311, -2.2470,\n",
      "         -2.2599, -2.2631],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2752, -2.3307, -2.3307, -2.2485,\n",
      "         -2.2610, -2.2628],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2756, -2.3303, -2.3303, -2.2477,\n",
      "         -2.2626, -2.2636],\n",
      "        [-2.3311, -2.3311, -2.3311, -2.3311, -2.2750, -2.3311, -2.3311, -2.2474,\n",
      "         -2.2603, -2.2627],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2744, -2.3304, -2.3304, -2.2481,\n",
      "         -2.2616, -2.2649],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2751, -2.3308, -2.3308, -2.2490,\n",
      "         -2.2616, -2.2615],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2743, -2.3303, -2.3303, -2.2496,\n",
      "         -2.2623, -2.2637],\n",
      "        [-2.3302, -2.3302, -2.3302, -2.3302, -2.2767, -2.3302, -2.3302, -2.2472,\n",
      "         -2.2617, -2.2647],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2754, -2.3304, -2.3304, -2.2491,\n",
      "         -2.2608, -2.2639],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2748, -2.3303, -2.3303, -2.2512,\n",
      "         -2.2609, -2.2629],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2759, -2.3307, -2.3307, -2.2469,\n",
      "         -2.2613, -2.2633],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2762, -2.3306, -2.3306, -2.2474,\n",
      "         -2.2611, -2.2632],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2751, -2.3304, -2.3304, -2.2484,\n",
      "         -2.2615, -2.2643],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2764, -2.3305, -2.3305, -2.2466,\n",
      "         -2.2611, -2.2646],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2748, -2.3306, -2.3306, -2.2479,\n",
      "         -2.2622, -2.2630],\n",
      "        [-2.3302, -2.3302, -2.3302, -2.3302, -2.2757, -2.3302, -2.3302, -2.2492,\n",
      "         -2.2619, -2.2636],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2754, -2.3306, -2.3306, -2.2483,\n",
      "         -2.2611, -2.2632],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2747, -2.3305, -2.3305, -2.2497,\n",
      "         -2.2612, -2.2628],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2754, -2.3306, -2.3306, -2.2493,\n",
      "         -2.2602, -2.2633],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2746, -2.3308, -2.3308, -2.2486,\n",
      "         -2.2606, -2.2634],\n",
      "        [-2.3310, -2.3310, -2.3310, -2.3310, -2.2738, -2.3310, -2.3310, -2.2490,\n",
      "         -2.2617, -2.2613],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2753, -2.3307, -2.3307, -2.2484,\n",
      "         -2.2600, -2.2635],\n",
      "        [-2.3310, -2.3310, -2.3310, -2.3310, -2.2747, -2.3310, -2.3310, -2.2484,\n",
      "         -2.2614, -2.2616],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2763, -2.3307, -2.3307, -2.2492,\n",
      "         -2.2607, -2.2613],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2750, -2.3305, -2.3305, -2.2494,\n",
      "         -2.2616, -2.2626],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2745, -2.3304, -2.3304, -2.2504,\n",
      "         -2.2614, -2.2631],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2752, -2.3306, -2.3306, -2.2487,\n",
      "         -2.2621, -2.2621],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2750, -2.3306, -2.3306, -2.2479,\n",
      "         -2.2621, -2.2628],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2762, -2.3305, -2.3305, -2.2486,\n",
      "         -2.2598, -2.2639],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2748, -2.3306, -2.3306, -2.2478,\n",
      "         -2.2624, -2.2632],\n",
      "        [-2.3302, -2.3302, -2.3302, -2.3302, -2.2774, -2.3302, -2.3302, -2.2483,\n",
      "         -2.2604, -2.2642],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2744, -2.3307, -2.3307, -2.2476,\n",
      "         -2.2623, -2.2632],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2752, -2.3303, -2.3303, -2.2483,\n",
      "         -2.2618, -2.2645],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2753, -2.3308, -2.3308, -2.2479,\n",
      "         -2.2611, -2.2627],\n",
      "        [-2.3299, -2.3299, -2.3299, -2.3299, -2.2755, -2.3299, -2.3299, -2.2492,\n",
      "         -2.2644, -2.2627],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2743, -2.3307, -2.3307, -2.2510,\n",
      "         -2.2608, -2.2613],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2745, -2.3308, -2.3308, -2.2472,\n",
      "         -2.2621, -2.2631],\n",
      "        [-2.3299, -2.3299, -2.3299, -2.3299, -2.2756, -2.3299, -2.3299, -2.2499,\n",
      "         -2.2611, -2.2652],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2737, -2.3308, -2.3308, -2.2508,\n",
      "         -2.2610, -2.2615],\n",
      "        [-2.3309, -2.3309, -2.3309, -2.3309, -2.2748, -2.3309, -2.3309, -2.2467,\n",
      "         -2.2619, -2.2628],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2752, -2.3304, -2.3304, -2.2495,\n",
      "         -2.2614, -2.2633],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2757, -2.3306, -2.3306, -2.2496,\n",
      "         -2.2611, -2.2618],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2756, -2.3305, -2.3305, -2.2480,\n",
      "         -2.2622, -2.2628],\n",
      "        [-2.3311, -2.3311, -2.3311, -2.3311, -2.2746, -2.3311, -2.3311, -2.2485,\n",
      "         -2.2602, -2.2621],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2755, -2.3304, -2.3304, -2.2486,\n",
      "         -2.2614, -2.2635],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2746, -2.3306, -2.3306, -2.2494,\n",
      "         -2.2614, -2.2630],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2749, -2.3303, -2.3303, -2.2493,\n",
      "         -2.2635, -2.2619],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2745, -2.3306, -2.3306, -2.2493,\n",
      "         -2.2620, -2.2620],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2748, -2.3306, -2.3306, -2.2485,\n",
      "         -2.2627, -2.2622],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2758, -2.3304, -2.3304, -2.2493,\n",
      "         -2.2613, -2.2625],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2739, -2.3306, -2.3306, -2.2501,\n",
      "         -2.2620, -2.2622],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2753, -2.3306, -2.3306, -2.2491,\n",
      "         -2.2599, -2.2637],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2749, -2.3307, -2.3307, -2.2483,\n",
      "         -2.2615, -2.2629],\n",
      "        [-2.3301, -2.3301, -2.3301, -2.3301, -2.2756, -2.3301, -2.3301, -2.2513,\n",
      "         -2.2615, -2.2623],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2759, -2.3307, -2.3307, -2.2494,\n",
      "         -2.2611, -2.2612],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2767, -2.3305, -2.3305, -2.2488,\n",
      "         -2.2611, -2.2623],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2736, -2.3307, -2.3307, -2.2498,\n",
      "         -2.2627, -2.2612],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2747, -2.3308, -2.3308, -2.2493,\n",
      "         -2.2615, -2.2613],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2757, -2.3303, -2.3303, -2.2495,\n",
      "         -2.2620, -2.2624],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2752, -2.3308, -2.3308, -2.2481,\n",
      "         -2.2607, -2.2629],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2748, -2.3307, -2.3307, -2.2499,\n",
      "         -2.2610, -2.2617],\n",
      "        [-2.3304, -2.3304, -2.3304, -2.3304, -2.2770, -2.3304, -2.3304, -2.2486,\n",
      "         -2.2599, -2.2636],\n",
      "        [-2.3299, -2.3299, -2.3299, -2.3299, -2.2773, -2.3299, -2.3299, -2.2484,\n",
      "         -2.2625, -2.2640],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2751, -2.3307, -2.3307, -2.2461,\n",
      "         -2.2624, -2.2637],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2738, -2.3305, -2.3305, -2.2508,\n",
      "         -2.2618, -2.2624],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2734, -2.3308, -2.3308, -2.2510,\n",
      "         -2.2614, -2.2610],\n",
      "        [-2.3311, -2.3311, -2.3311, -2.3311, -2.2748, -2.3311, -2.3311, -2.2472,\n",
      "         -2.2601, -2.2632],\n",
      "        [-2.3308, -2.3308, -2.3308, -2.3308, -2.2751, -2.3308, -2.3308, -2.2483,\n",
      "         -2.2612, -2.2627],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2749, -2.3307, -2.3307, -2.2493,\n",
      "         -2.2613, -2.2619],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2765, -2.3305, -2.3305, -2.2471,\n",
      "         -2.2619, -2.2630],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2756, -2.3306, -2.3306, -2.2496,\n",
      "         -2.2607, -2.2623],\n",
      "        [-2.3309, -2.3309, -2.3309, -2.3309, -2.2751, -2.3309, -2.3309, -2.2478,\n",
      "         -2.2606, -2.2629],\n",
      "        [-2.3310, -2.3310, -2.3310, -2.3310, -2.2748, -2.3310, -2.3310, -2.2467,\n",
      "         -2.2614, -2.2629],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2743, -2.3307, -2.3307, -2.2485,\n",
      "         -2.2614, -2.2631],\n",
      "        [-2.3310, -2.3310, -2.3310, -2.3310, -2.2751, -2.3310, -2.3310, -2.2487,\n",
      "         -2.2595, -2.2623],\n",
      "        [-2.3311, -2.3311, -2.3311, -2.3311, -2.2744, -2.3311, -2.3311, -2.2471,\n",
      "         -2.2613, -2.2623],\n",
      "        [-2.3305, -2.3305, -2.3305, -2.3305, -2.2753, -2.3305, -2.3305, -2.2486,\n",
      "         -2.2614, -2.2632],\n",
      "        [-2.3303, -2.3303, -2.3303, -2.3303, -2.2745, -2.3303, -2.3303, -2.2515,\n",
      "         -2.2608, -2.2628],\n",
      "        [-2.3307, -2.3307, -2.3307, -2.3307, -2.2753, -2.3307, -2.3307, -2.2485,\n",
      "         -2.2612, -2.2625],\n",
      "        [-2.3306, -2.3306, -2.3306, -2.3306, -2.2750, -2.3306, -2.3306, -2.2494,\n",
      "         -2.2606, -2.2633]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-b3c0060c2185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m    896\u001b[0m     def __init__(self, weight=None, size_average=None, ignore_index=-100,\n\u001b[1;32m    897\u001b[0m                  reduce=None, reduction='mean'):\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msize_average\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "epochs = 100\n",
    "y_pred = []\n",
    "for epoch in range(epochs):\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        pred = obj(images)\n",
    "        \n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:                              \n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'%(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
