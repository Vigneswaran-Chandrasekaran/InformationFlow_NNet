Unsupervised Bin-wise Pre-training: A Fusion of Information Theoretic and Hypergraph Concepts

Abstract:
Minimizing the training time of the Deep Neural Networks still remains a significant
challenge, as the parameters are huge. There arises the need for optimizing and
regularizing the parameters within minimal time. To achieve this, pre-training is one of
the promising technique which is widely preferred by the researchers and considered to
be a ‘starting point’ of the Deep Neural Networks. Plenty of pre-training models are
presented in the recent research works however they often fail to capture the relevant
information and to maintain the stability of the learning model. Hence, this research
article presents a novel unsupervised bin-wise pre-training model which fuses
Information Theory, Partial Information Decomposition and Hypergraph concepts to
speed up the learning process and to minimize the training &amp; validation loss of the Deep
Neural Networks through improved feature representation. Further, a new approach of
parameter updation during pre-training has been introduced that acts both as an
optimizer &amp; a regularizer. The proposed model has been evaluated using MNIST
benchmark image dataset and the experimental results confirm the effectiveness of the
proposed unsupervised bin-wise pre-training model in terms of regularization &amp;
optimization capability and achieves competitive results compared to the state-of-the-
art approaches.
