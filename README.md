# Unsupervised binwise pre-training

Minimizing the training time of the Deep Neural Networks still remains a significant challenge, as the parameters are huge. There arises the need for optimizing and regularizing the parameters within minimal time.  To achieve this, pre-training is one of the promising technique which is widely preferred by the researchers and considered to be a ‘starting point’ of the Deep Neural Networks. Plenty of pre-training models are presented in the recent research works however they often fail to capture the relevant information and to maintain the stability of the learning model. Hence, this research article presents a novel unsupervised bin-wise pre-training model which fuses Information theory and Hypergraph to speed up the learning process and to minimize the training & validation loss of the Deep Neural Networks through improved feature representation. Further, a new quantity termed as divergence factor (ϱ) has been introduced that acts both as an optimizer & a regularizer and the K-Helly property of hypergraph has been employed. The proposed model has  been evaluated using MNIST benchmark dataset and the experimental results confirms the effectiveness of the proposed bin-wise pre-training model in terms of performance graphs and achieves competitive results compared to the state-of-the-art approaches. 

Introduction: 
Deep Learning (DL) - an integral part of neural network guarantees higher accuracy and flexibility by learning to represent the world as a nested hierarchy of Information, with each defined in relation to simpler ones [1]. Powerful features of DL such as an increase in robustness and performance of the model as the data increases, learning higher-level features from the data incrementally without feature engineering, end-to-end problem-solving capability, etc., make four among five researchers believe that the advent of DL makes life easier [2]. However, parameter initialization is one of the major issues of DL as it affects the speed of convergence and the generalization of the model [3–10]. To address this issue, pre-training is widely adapted in DL as it helps in finding a better starting point in loss topology for improved Empirical Risk Minimization [11]. Pre-training is a process of adding new hidden layers for constructing a deep learning model and overhauling, permitting the newly added layer to acquire the information from the preceding hidden layers [12]. Particularly, Unsupervised Pre-training focuses on weight updation for effective Information feature transformation and representation through layers, which reduces the high time-consuming exploration phase of the Optimization algorithm [13]. Among the existing unsupervised pre-training approaches Deep Belief Networks (DBN), Autoencoders and its variants were extensively used for pre-training [14]. However, the existing pre-training strategies suffer due to computational complexity and perform compression rather conceptualization.On the other side, recent research works on understanding unfathomable concepts of DL [15–17] and developing various models [18–20] to improve state-of-the-art methods through Information Theory-based approach has proven successful.  Information Theory discloses how parameters are motivated to acquire the information from the known data and plausibly able to expound the trends observed during training[20]. This different perspective of viewing Deep Learning model helps us to answer, how model proceeds to optimize instead of a stochastic step. However, a very few have attempted to use Information Theory to solve the limitations of pre-training and lacks to exploit the complete significance of Information Theory.To overcome the aforementioned issues, this paper proposes a novel pre-training model which fuses information theory and hypergraph for the Deep Neural Network in an unsupervised fashion. 
