{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bin_wise_pretraining.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG7GnQAZ-RjI",
        "colab_type": "code",
        "outputId": "8448c268-e094-48ad-eafc-b0b0ad4aa7d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install fast-histogram\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fast-histogram\n",
            "  Downloading https://files.pythonhosted.org/packages/86/fa/88e1193537d18986dcbd1f6cdf9f0475b0588345722f4528c1aa8d93f1a6/fast_histogram-0.7-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fast-histogram) (1.16.4)\n",
            "Installing collected packages: fast-histogram\n",
            "Successfully installed fast-histogram-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4Ha_KN0MZ6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "from scipy.stats import entropy\n",
        "from torch.utils.data import DataLoader\n",
        "from fast_histogram import histogram1d, histogram2d\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dataset\n",
        "import torch.nn.functional as fun\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch\n",
        "import time\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class DeepNN(nn.Module):\n",
        "    # Class defining the structure of Deep Neural Network model and layers characterstics\n",
        "    def __init__(self, input_dim, nh1, nh2, nh3, nh4, nh5, output_dim):\n",
        "        super(DeepNN, self).__init__()\n",
        "        #hyperparameter setting \n",
        "        self.input_dim = input_dim\n",
        "        self.nh1, self.nh2, self.nh3, self.nh4, self.nh5, = nh1, nh2, nh3, nh4, nh5\n",
        "        self.output_dim = output_dim\n",
        "        #layer definition \n",
        "        torch.nn.init.kaiming_uniform()\n",
        "        self.input_layer = nn.Linear(self.input_dim, self.nh1)\n",
        "        self.hlayer1 = nn.Linear(self.nh1, self.nh2)\n",
        "        self.hlayer2 = nn.Linear(self.nh2, self.nh3)\n",
        "        self.hlayer3 = nn.Linear(self.nh3, self.nh4)\n",
        "        self.hlayer4 = nn.Linear(self.nh4, self.nh5)\n",
        "        self.output_layer = nn.Linear(self.nh5, self.output_dim)\n",
        "    # function to simulate forward propogation\n",
        "    def forward(self, x):\n",
        "        #propogation of each layer\n",
        "        self.out1 = fun.relu(self.input_layer(x))\n",
        "        self.out2 = fun.relu(self.hlayer1(self.out1))\n",
        "        self.out3 = fun.relu(self.hlayer2(self.out2))\n",
        "        self.out4 = fun.relu(self.hlayer3(self.out3))\n",
        "        self.out5 = fun.relu(self.hlayer4(self.out4))\n",
        "        self.out6 = fun.relu(self.output_layer(self.out5))\n",
        "        #output layer\n",
        "        return fun.softmax(self.out6, dim = 1)\n",
        "\n",
        "#@profile     # to see the memory profile of the function uncomment the  @profile decorator\n",
        "def dataset_load(tr_batch_size, val_batch_size, val_split):\n",
        "    #Download and prepare dataset chunks by DataLoader\n",
        "    tic = time.time()\n",
        "    print(\"Downloading dataset and preparing DataLoader\")\n",
        "    master_dataset = dataset.MNIST(root = './data', train = True, transform = transforms.ToTensor(), download = True )\n",
        "    test_dataset = dataset.MNIST(root = './data', train = False, transform = transforms.ToTensor())\n",
        "    #Train and validation data is split with specified ratio\n",
        "    train_dataset, val_dataset = data.random_split(master_dataset, (int(len(master_dataset)*(1.0-val_split)), int(len(master_dataset)*val_split)))\n",
        "    #define dataloaders with defined batch size for training and validation\n",
        "    train_loader = DataLoader(dataset = train_dataset, batch_size = tr_batch_size, shuffle = True)\n",
        "    # validation data is shuffled as validation set is used in pretraining and so to avoid any particular class bias\n",
        "    val_loader = DataLoader(dataset = val_dataset, batch_size = val_batch_size, shuffle = True)\n",
        "    # shuffling test dataset is not required\n",
        "    test_loader = DataLoader(dataset = test_dataset, batch_size = tr_batch_size, shuffle = False)\n",
        "    toc = time.time()\n",
        "    print(\"Finished preparing. Total time elasped: \"+str(toc - tic)+\" seconds\")\n",
        "    return( train_loader, val_loader, test_loader)\n",
        "\n",
        "def estimate_mutual_info(X, neurons, bins = 5):\n",
        "    #Estimate Mutual Information between Input data X and Neuron's activations\n",
        "    neuronal_MI = np.zeros(neurons.shape[1])\n",
        "    index = 0\n",
        "    print(\"Number of neurons computed:\")\n",
        "    for neuron in neurons.T:\n",
        "        if (index+1) % 100 == 0:\n",
        "            print(\"#####\"+str(index+1), end = \"\", flush = True)\n",
        "        #loop over each neuron \n",
        "        # neuron is the activation of particular neuron for each input data X\n",
        "        for dim in X.T:            \n",
        "            # loop over each dimension to estimate MI\n",
        "            # we assume each dimension is independent to each other..\n",
        "            # Ref: https://stats.stackexchange.com/questions/413511/mutual-information-between-multi-dimensional-and-single-dimensional-variables      \n",
        "            if np.amax(dim) != np.amin(dim):\n",
        "                #check whether the dimension have only one value throughout \n",
        "                #normalize the values for faster computation\n",
        "                dim = (dim - np.amin(dim)) / (np.amax(dim) - np.amin(dim))\n",
        "                neuron = (neuron - np.amin(neuron)) / (np.amax(neuron) - np.amin(neuron))\n",
        "                #build histogram for joint X and Y\n",
        "                bins_xy = histogram2d(dim, neuron, bins, range = [[0,1],[0,1]])\n",
        "                #histogram for X and Y marginal\n",
        "                bins_x = histogram1d(dim, bins, range = [0,1])\n",
        "                bins_y = histogram1d(neuron, bins, range = [0,1])\n",
        "                # check any sum is zero, although previos condition checks.. a defensive program\n",
        "                if np.sum(bins_x) != 0 and np.sum(bins_y) != 0 and np.sum(bins_xy) != 0:\n",
        "                    #calculate marginal probabilities\n",
        "                    p_x = bins_x / np.sum(bins_x)\n",
        "                    p_y = bins_y / np.sum(bins_y)\n",
        "                    #calculate joint probability\n",
        "                    p_xy = bins_xy / np.sum(bins_xy)\n",
        "                    #estimate entropy \n",
        "                    H_x = -1 * np.sum( p_x * np.log(p_x))\n",
        "                    H_y = -1 * np.sum(p_y * np.log(p_y))\n",
        "                    H_xy = -1 * np.sum(p_xy * np.log(p_xy))\n",
        "                    #Mutual Information of the particular dimension and neuron out put \n",
        "                    # I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
        "                    sum = H_x + H_y - H_xy\n",
        "                    # check any is NaN.. This occurs sometimes because, in p_X * log(p_X), sometimes p_X becomes 0\n",
        "                    # making the whole term zero, but however in numpy this turns to become NaN...\n",
        "                    # TODO: Find a good way to handle this\n",
        "                    if not math.isnan(sum):\n",
        "                        # add the MI value to the corresponding neuron index\n",
        "                        neuronal_MI[index] += sum\n",
        "        index += 1    \n",
        "    # return array of mutual information corresponding to each neuron\n",
        "    return(neuronal_MI)\n",
        "\n",
        "def check_k_strong_helly():\n",
        "    # Using k-helly property to check wheteher the stopping condition is satisfied or not\n",
        "    return False\n",
        "\n",
        "  #@profile\n",
        "def pre_train_model(model, val_loader):\n",
        "    # list of all model's hidden layers\n",
        "    layers = [model.input_layer, model.hlayer1, model.hlayer2, model.hlayer3, model.hlayer4]\n",
        "    # Hyperparameter k-value which determines number of bins for the particular layer\n",
        "    # This is calaculated based on Partial Information Decomposition \n",
        "    k_value = [5, 10, 4, 4, 2]\n",
        "    # k-helly stopping criterion's k-value to chec degree of overlap\n",
        "    k_helly = [5, 5, 5, 5, 5]\n",
        "    # loop over the layers \n",
        "    for l_indx in range(len(layers)):\n",
        "        # get the layer's parameters and detach it for updation \n",
        "        print(\"Working on layer: \"+str(l_indx))\n",
        "        w_matrix = layers[l_indx].weight.data.clone().detach().numpy()\n",
        "        b_matrix = layers[l_indx].bias.data.clone().detach().numpy()\n",
        "        #load the input data X\n",
        "        # use the validation dataset images as X\n",
        "        for _, (images, _) in enumerate(val_loader):\n",
        "            global actX, act1, act2, act3, act4\n",
        "            # reshape images \n",
        "            images = images.reshape(-1, 28*28).clone().detach().numpy()\n",
        "            # find the activaion by,\n",
        "            # act( X.W_T + b)\n",
        "            # We need to freeze each layer one by one and use the updated weights\n",
        "            # from before layer, hence we have the immmediate before data as actX\n",
        "            # When the paricular layer is called, subsequent sequential of outputs\n",
        "            # are calculated and final activation is calculated\n",
        "            if l_indx == 0:    # First layer, which have actX as input images [immediate input to the layer]\n",
        "                actX = images   \n",
        "            elif l_indx == 1:  # Second layer\n",
        "                layer1_param = np.load('Param_1024_1_divergence.npz') #load the saved updated weights and biases\n",
        "                # act1 contains the activations when the weights are updated, this acts as a input\n",
        "                # for subsequent layers\n",
        "                act1 = 1 / (1 + np.exp(-1 * (np.dot(images, layer1_param['w'].T) + layer1_param['b'])))  \n",
        "                actX = act1\n",
        "            elif l_indx == 2:  # Third layer\n",
        "                layer1_param = np.load('Param_1024_1_divergence.npz') #load the saved updated weights and biases\n",
        "                act1 = 1 / (1 + np.exp(-1 * (np.dot(images, layer1_param['w'].T) + layer1_param['b'])))  \n",
        "                layer2_param = np.load('Param_1024_2_divergence.npz')\n",
        "                act2 = 1 / ( 1 + np.exp(-1 * np.dot(act1, layer2_param['w'].T) + layer2_param['b']))\n",
        "                actX = act2\n",
        "            elif l_indx == 3:  # Fourth layer\n",
        "                layer1_param = np.load('Param_1024_1_divergence.npz') #load the saved updated weights and biases\n",
        "                act1 = 1 / (1 + np.exp(-1 * (np.dot(images, layer1_param['w'].T) + layer1_param['b'])))  \n",
        "                layer2_param = np.load('Param_1024_2_divergence.npz')\n",
        "                act2 = 1 / ( 1 + np.exp(-1 * np.dot(act1, layer2_param['w'].T) + layer2_param['b']))\n",
        "                layer3_param = np.load('Param_1024_3_divergence.npz')\n",
        "                act3 = 1 / (1 + np.exp(-1 * (np.dot(act2,layer3_param['w'].T) + layer3_param['b'])))\n",
        "                actX = act3\n",
        "            elif l_indx == 4:   # Fifth layer\n",
        "                layer1_param = np.load('Param_1024_1_divergence.npz') #load the saved updated weights and biases\n",
        "                act1 = 1 / (1 + np.exp(-1 * (np.dot(images, layer1_param['w'].T) + layer1_param['b'])))  \n",
        "                layer2_param = np.load('Param_1024_2_divergence.npz')\n",
        "                act2 = 1 / ( 1 + np.exp(-1 * np.dot(act1, layer2_param['w'].T) + layer2_param['b']))\n",
        "                layer3_param = np.load('Param_1024_3_divergence.npz')\n",
        "                act3 = 1 / (1 + np.exp(-1 * (np.dot(act2,layer3_param['w'].T) + layer3_param['b'])))\n",
        "                layer4_param = np.load('Param_1024_4_divergence.npz')\n",
        "                act4 = 1 / (1 + np.exp(-1 * (np.dot(act3,layer4_param['w'].T) + layer4_param['b'])))\n",
        "                actX = act4\n",
        "            # get the activation of the layer when actX , w_matrix and b_matrix are known\n",
        "            #activation = 1 / (1 + np.exp(-1 * (np.dot(actX,w_matrix.T) + b_matrix )))\n",
        "            activation = np.tanh(np.dot(actX,w_matrix.T) + b_matrix )\n",
        "            # Estimate Mutual Information for each neuron\n",
        "            tic_est = time.time()\n",
        "            print(\"Estimation Information Theorotic quantities\")\n",
        "            neuronal_MI = estimate_mutual_info(images, activation, bins = 5)\n",
        "            master_neuron.append(neuronal_MI)\n",
        "            toc_est = time.time()\n",
        "            print(\"Elasped time for estimation: \"+str(round(toc_est-tic_est,2))+\" seconds\")\n",
        "            print(\"----- This elasped time is commenstruate with time required for the particular layer -----\")\n",
        "            # Get the index of sorted neurons based on MI value\n",
        "            index_sorted = np.argsort(neuronal_MI)[::-1]\n",
        "            #create clusters of given k value\n",
        "            clusters = np.array_split(index_sorted, k_value[l_indx])\n",
        "            # calculate the bin's avergage MI\n",
        "            bin_avg = []\n",
        "            for i in clusters:\n",
        "                bin_avg.append(np.sum(neuronal_MI[np.ix_(i)]) / neuronal_MI[np.ix_(i)].shape[0])\n",
        "            # start tuning parameters !!!\n",
        "            # hyperparameters defined \n",
        "            iteration = 0\n",
        "            decay_factor = 0.0001    # decay factor for step-size\n",
        "            stopping_criteria = True\n",
        "            while stopping_criteria:\n",
        "                print(\"Iteration: \"+str(iteration))\n",
        "                for bin in range(len(clusters)):\n",
        "                    #TODO: parameter updation very important.... High priority    \n",
        "                    # w_matrix have n number of rows with each row representing particular neuron connection\n",
        "                    # Divergence between current bin and best bin(target)\n",
        "                    divergence = entropy(clusters[bin_avg.index(max(bin_avg))], clusters[bin])\n",
        "                    if math.isinf(divergence):\n",
        "                      divergence = 1\n",
        "                    print(\"Divergence: \"+str(divergence))\n",
        "                    w_matrix[np.ix_(clusters[bin])] -=  divergence * (1/(iteration+1))*decay_factor\n",
        "                    b_matrix[np.ix_(clusters[bin])] -=  divergence * (1/(iteration+1))*decay_factor\n",
        "                iteration += 1\n",
        "                if iteration == 5 or check_k_strong_helly():   #TODO: define function to check k-helly property criteria\n",
        "                    if iteration == 5:\n",
        "                        print(\"Number of Iterations == Max_iteration!\")\n",
        "                    else:\n",
        "                        print(\"K-Helly property is satisfied!\")\n",
        "                    print(\"Finish training the layer\"+str(l_indx))\n",
        "                    stopping_criteria = False\n",
        "                    layers[l_indx].weight.data = torch.tensor(w_matrix)\n",
        "                    layers[l_indx].weight.data = torch.tensor(b_matrix)\n",
        "                    file_name = input(\"Enter the file name to save the layer's parameters: \")\n",
        "                    np.savez(\"file_name\", w = w_matrix, b = b_matrix)\n",
        "                #calculate activations for the updated weights \n",
        "                activation = np.tanh(np.dot(actX,w_matrix.T) + b_matrix )\n",
        "                #estimate MI for the tuned parameters\n",
        "                neuronal_MI_after_update = estimate_mutual_info(images, activation, bins = 5)\n",
        "                index_sorted_after_update = np.argsort(neuronal_MI_after_update)[::-1]\n",
        "                clusters_after_update = np.array_split(index_sorted_after_update, k_value[l_indx])\n",
        "                # calculate the bin's avergage MI\n",
        "                bin_avg_after_update = []\n",
        "                for i in clusters_after_update:\n",
        "                    bin_avg_after_update.append(np.sum(neuronal_MI_after_update[np.ix_(i)]) / neuronal_MI_after_update[np.ix_(i)].shape[0])\n",
        "                clusters = clusters_after_update\n",
        "                bin_avg = bin_avg_after_update\n",
        "                neuronal_MI = neuronal_MI_after_update  \n",
        "                \n",
        "    return(model)\n",
        "\n",
        "if __name__ == '__main__':        \n",
        "\n",
        "    tr_batch_size = 4800\n",
        "    val_batch_size = 12000\n",
        "    val_split = 0.2\n",
        "    train_loader, val_loader, test_loader = dataset_load(tr_batch_size, val_batch_size, val_split)\n",
        "    actX = 0; act1 = 0; act2 = 0; act3 = 0; act4 = 0\n",
        "    model = DeepNN(784, 1024, 200, 20, 20, 20, 10)\n",
        "    print(\"Pretraining phase..\")\n",
        "    pre_trained_model = pre_train_model(model, val_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}